{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext \n",
    "\n",
    "  \n",
    "\n",
    "# use to find bigrams, which are pairs of words \n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "\n",
    "from nltk.metrics import BigramAssocMeasures \n",
    "\n",
    "​\n",
    "\n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    return words_freq[:n]\n",
    "\n",
    "common_words = get_top_n_bigram(df['Review Text'], 20)\n",
    "\n",
    "for word, freq in common_words:\n",
    "\n",
    "    print(word, freq)\n",
    "\n",
    "df4 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "\n",
    "df4.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "\n",
    "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in review after removing stop words')\n",
    "\n",
    "view raw\n",
    "\n",
    "top_bigram_no_stopwords.py hosted with ❤ by GitHub \n",
    "\n",
    "biagram_collocation = BigramCollocationFinder.from_words(text_100) \n",
    "\n",
    "biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 15) \n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "import cv2def \n",
    "\n",
    "create_mask():\n",
    "\n",
    "    mask = np.array(Image.open(\"coronavirus.png\"))\n",
    "\n",
    "    im_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    _, mask = cv2.threshold(im_gray, thresh=20, maxval=255, type=cv2.THRESH_BINARY)\n",
    "\n",
    "    mask = 255 - mask\n",
    "\n",
    "    return maskmask = create_mask()def create_wc_by(source):\n",
    "\n",
    "    data = fulldf[fulldf['source']==source]\n",
    "\n",
    "    text = \" \".join([x for x in data.content.values if x is not None])\n",
    "\n",
    "    stopwords = set(STOPWORDS)\n",
    "\n",
    "    stopwords.add('chars')\n",
    "\n",
    "    stopwords.add('coronavirus')\n",
    "\n",
    "    stopwords.add('corona')\n",
    "\n",
    "    stopwords.add('chars')\n",
    "\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000, mask=mask, stopwords=stopwords,\n",
    "\n",
    "               max_font_size=90, random_state=42, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "    wc.generate(text)\n",
    "\n",
    "    plt.figure(figsize=[30,30])\n",
    "\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    return pltst.pyplot(create_wc_by(source),use_container_width=True)\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "print (join(list(chain.from_iterable(filtered_sentence))))\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "vect.set_params(tokenizer=tokenizer.tokenize(p2.text))\n",
    "\n",
    "​\n",
    "\n",
    "# remove English stop words\n",
    "\n",
    "vect.set_params(stop_words='english')\n",
    "\n",
    "​\n",
    "\n",
    "# include 1-grams and 2-grams\n",
    "\n",
    "vect.set_params(ngram_range=(1, 2))\n",
    "\n",
    "​\n",
    "\n",
    "# ignore terms that appear in more than 50% of the documents\n",
    "\n",
    "vect.set_params(max_df=0.5)\n",
    "\n",
    "​\n",
    "\n",
    "# only keep terms that appear in at least 2 documents\n",
    "\n",
    "vect.set_params(min_df=2)\n",
    "\n",
    "vect\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "tokenizer.tokenize(text_100.text)\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(text_100.text)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "print(stop_words)\n",
    "\n",
    "text_100_nltk = []\n",
    "\n",
    "for word in tokens:\n",
    "\n",
    "    if word not in stop_words:\n",
    "\n",
    "        filtered_sentence.append(word)\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "print(\"=================================================================================\")\n",
    "\n",
    "print(filtered_sentence)\n",
    "\n",
    "list(nltk.FreqDist(filtered_sentence).values())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import webtext \n",
    "\n",
    "  \n",
    "\n",
    "# use to find bigrams, which are pairs of words \n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder \n",
    "\n",
    "from nltk.metrics import BigramAssocMeasures \n",
    "\n",
    "​\n",
    "\n",
    "def get_top_n_bigram(corpus, n=None):\n",
    "\n",
    "    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)\n",
    "\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "\n",
    "    return words_freq[:n]\n",
    "\n",
    "common_words = get_top_n_bigram(df['Review Text'], 20)\n",
    "\n",
    "for word, freq in common_words:\n",
    "\n",
    "    print(word, freq)\n",
    "\n",
    "df4 = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "\n",
    "df4.groupby('ReviewText').sum()['count'].sort_values(ascending=False).iplot(\n",
    "\n",
    "    kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in review after removing stop words')\n",
    "\n",
    "view raw\n",
    "\n",
    "top_bigram_no_stopwords.py hosted with ❤ by GitHub \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
